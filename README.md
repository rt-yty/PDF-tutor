Этот проект представляет собой RAG (Retrieval-Augmented Generation) приложение, которое позволяет загружать PDF-документы и задавать по ним вопросы. Приложение использует языковые модели из Hugging Face для понимания текста и генерации ответов на основе содержимого загруженных файлов.
## Технологии
- **Бэкенд:** FastAPI для создания асинхронных API и Uvicorn в качестве ASGI-сервера.
- **Обработка данных:** LangChain для оркестрации работы с документами и моделями.
- **Работа с PDF:** PyPDF для загрузки и извлечения текста из PDF-документов.
- **Векторное хранилище:** FAISS для хранения эмбеддингов и быстрого поиска по схожести.
- **Модели и Эмбеддинги:**
    - Hugging Face Hub для доступа к предобученным моделям.
    - Sentence Transformers для создания векторных представлений (эмбеддингов) текста.

- **Контейнеризация:** Docker и Docker Compose для простоты развертывания и изоляции окружения.

## Структура проекта
``` 
.
├── app/
│   ├── api/          # Модули API с эндпоинтами FastAPI
│   ├── core/         # Конфигурация приложения
│   ├── services/     # Бизнес-логика
│   ├── data/         # Директория для хранения PDF и индекса FAISS (создается автоматически)
│   └── main.py       # Основной файл приложения FastAPI
├── .dockerignore
├── .gitignore
├── docker-compose.yml
├── Dockerfile
└── requirements.txt
```

## Настройка и запуск
### 1. Переменные окружения
Создайте файл в корневом каталоге проекта и добавьте в него следующие переменные: `.env`
``` env
# Токен для доступа к Hugging Face API
HF_TOKEN="ваш_токен_hugging_face"

# ID модели для генерации ответов (например, "mistralai/Mistral-7B-Instruct-v0.2")
HF_MODEL_ID="ID_вашей_модели"

# Модель для создания эмбеддингов (например, "sentence-transformers/all-MiniLM-L6-v2")
EMBEDDING_MODEL="ID_модели_эмбеддингов"

# Параметры для разбиения текста на чанки
# CHUNK_SIZE=1000
# CHUNK_OVERLAP=200

# Количество документов для поиска по схожести
# K_DEFAULT=4

# --- Опциональные переменные ---

# Директория для хранения загруженных PDF и индекса FAISS
# DATA_DIR="app/data"
# INDEX_PATH="app/data/faiss_index"

```
### 2. Запуск с помощью Docker
Это рекомендуемый способ запуска приложения.
1. **Сборка и запуск контейнеров:** Выполните команду в корневой директории проекта:
``` bash
    docker-compose up --build -d
```
1. Приложение будет доступно по адресу `http://localhost:8000`.

### 3. Локальный запуск (без Docker)
1. **Установка зависимостей:**
``` bash
    pip install -r requirements.txt
```
1. **Запуск приложения:**
``` bash
    uvicorn app.main:app --reload
```
## Как использовать
Вы можете взаимодействовать с API через документацию Swagger, доступную по адресу `http://localhost:8000/docs`.
### 1. Загрузка PDF-документа
- **Эндпоинт:** `/ingest`
- **Метод:** `POST`
- **Тело запроса:** `multipart/form-data`, где ключ — `file`, а значение — ваш PDF-файл.

Этот эндпоинт обработает ваш PDF: разделит его на части, создаст эмбеддинги и сохранит их в векторном хранилище FAISS.
### 3. Просмотр списка загруженных документов
- **Эндпоинт:** `/docs/`
- **Метод:** `GET`

Возвращает список всех PDF-файлов, которые были загружены в систему.
### 4. Удаление документов и индекса
- **Эндпоинт:** `/maintenance/reset-all`
- **Метод:** `DELETE`

Этот эндпоинт полностью очищает хранилище: удаляет все загруженные PDF-файлы и индекс векторного хранилища FAISS.
### 2. Ответы на вопросы
- **Эндпоинт:** `/ask`
- **Метод:** `POST`
- **Тело запроса (JSON):**
``` json
    {
      "question": "Ваш вопрос по документу?",
      "k": 4
    }
```
- `question` (str): Ваш вопрос.
- `k` (int, опционально): Количество наиболее релевантных фрагментов текста, которые будут использованы для генерации ответа.